{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7befc7ec",
   "metadata": {},
   "source": [
    "# Case Study: Contrail Segmentation \n",
    "## On the Importance of Understanding Your Data and Task\n",
    "*Rolls off the tongue, doesn't it?* - The Authors of This Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08c90e",
   "metadata": {},
   "source": [
    "This notebook presents a set of exercises in the form of a case study on segmenting contrails from satellite image (also referred to as remote sensing data). \n",
    "\n",
    "*Prerequisties*:\n",
    "- Basic machine learning knowledge \n",
    "- Basic familiarity with the paper \"Few-Shot Contrail Segmentation in Remote Sensing Imagery With Loss Function in Hough Space\" by Junzi et al. (https://ieeexplore.ieee.org/document/10820969). We do not expect you to have read the whole paper, but rather to have an idea of what is being done.\n",
    "\n",
    "*The goal by the end of this notebook is for students to:*\n",
    "- Gain an exposure to a new application of artificial intelligence - extracing semantics from satellite images \n",
    "- Understand when they can consider the specifics of their data and task\n",
    "- Gain exposure to methods to take advantage of their task\n",
    "- Understand how to evaluate machine learning tasks based on the goal they are solving \n",
    "- Go beyond the technical understanding of the problem and consider where this application will be used, will it even be helpful, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7518a7a3",
   "metadata": {},
   "source": [
    "## Introduction: Contail Segmentation\n",
    "The paper \"Few-Shot Contrail Segmentation in Remote Sensing Imagery With Loss Function in Hough Space\" by Junzi et al. focuses on creating an automatic segmentation procedure for contrails when using few data samples by taking advantage of what we know about the problem. In this section, the problem is introduced from the very basics of what a contrails is, to a mathematical formulation of the issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc898b0f",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images/what-is-a-contrail.png\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "*What is a contrail?* \n",
    "\n",
    "Contrails or vapour trails are line-shaped clouds produced by aircraft engine exhaust or changes in air pressure, typically at aircraft cruising altitudes several kilometres/miles above the Earth's surface. (Definition - https://en.wikipedia.org/wiki/Contrail)\n",
    "\n",
    "*Why does it matter?* \n",
    "\n",
    "The Sun emits solar radiation towards the Earth that the ground traditionally reflects back. However, the formation of the ice crystals in contrails creates a dense enough \"shield\" to reflect a part of them back. This impacts the amount of radiation trapped around the Earth and thus contributes to the temperature. They also have a potential cooling effect on sun rays getting reflected back towards the Sun, though this effect is currently estimated to be smaller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31becf5",
   "metadata": {},
   "source": [
    "**Satellite Image** \n",
    "\n",
    "Satellite images are a unique form of data. As the name implies, they are taken by a satellite, which leads to charactersitics about it, such as:\n",
    "- **top-down**: the perspective of the image is always orthogonal towards the ground \n",
    "- **distance from the Earth**: Depending on the image, and satellite both the distance to the Earth and the resolution of the image is different. \n",
    "    - For this reason, the resolution is measured in terms of actual distance (e.g. A resolution of 30cm means that 30cm of information is encoded per pixel.)\n",
    "    - Common resolutions are 30cm, 1m, 2m. Correspondingly a smaller distance corresponds to higher quality, more expense, harder to get, etc.\n",
    "- **multispectral data**: Sensory information from satellites is more advanced than traditional cameras. They can capture more than just the color spectrum. Each one is a special band (channel in images). Below are listed some examples: \n",
    "    - RGB (Red-Green-Blue): The channels traditional cameras capture, and are blended together to create a final image\n",
    "    - Infrared/Near-Infrared: A channel where some surfaces react differently to - for example, vegetation reacts differently to infrared light. Can reveal new information, not visible to the naked eye.\n",
    "    - Point Clouds (LiDAR): A channel capturing distance to the earth. Used to map out things like elevation via the laser reflecting back. \n",
    "    - And many others, depending on the sensor used ... We will introduce them in this notebook if and as necessary.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "   <img src=\"images/remote-sensing-platforms.webp\" width=\"450\"/>\n",
    "\n",
    "</div>\n",
    "\n",
    "[Image Source: GeeksForGeeks](https://imgs.search.brave.com/ebFlnbhNCKYFr760b0JRrC-BQlpJUeTmlB5SUw-5Nf4/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9tZWRp/YS5nZWVrc2ZvcmdZWtzLm9yZy93cC)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Below are listed some example images of contrail images taken from different satellites and bands: \n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "  <img src=\"images/1-MeteoSat 11.png\" width=\"350\"/>\n",
    "  <img src=\"images/2-NASA Terra.jpg\" width=\"350\"/>\n",
    "  <img src=\"images/3-NOAA Suomi-NPP.jpg\" width=\"350\"/>\n",
    "</div>\n",
    "\n",
    "[Image Source TBD](TBD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02d044",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab45ba0",
   "metadata": {},
   "source": [
    "### Create an Anaconda Environment and Download Requirements\n",
    "If you haven't already setup the codebase following the instructions of the README, you can do so here. Otherwise, you can skip running this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda create -y -n contrail-project python=3.12\n",
    "%conda activate contrail-project\n",
    "%conda install -y pip\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55189639",
   "metadata": {},
   "source": [
    "### Get Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    # Force deterministic behavior in cudnn (might slow things down)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271b870",
   "metadata": {},
   "source": [
    "## What is Contrail Segmentation?\n",
    "\n",
    "What are we trying to accomplish? From the name **contrail segmentation**, and the information beforehand, we know that we are trying to extract information out of the satellite images. To do so, we need to define **segmentation** first.\n",
    "\n",
    "#### Segmentation\n",
    "\n",
    "Recall **classification**, abstracted from any machine learning model. Given an input $x$, we want to get an output $\\widehat y$ corresponding to the correct class $y$ of the object $x$. \n",
    "\n",
    "In remote sensing, our inputs are images. Therefore $x \\in \\mathbb R^{W \\times H \\times C} $ where $W, H$ are the height and width (in pixels) of the current image and $C$ the number of channels total the image (recall RGB or Infrared introduced previously). Mathematically, this is a **tensor*(it is not very important for today, but good to know in general). \n",
    "\n",
    "In **segmentation**, we want to semantically understand our image. A way to do this is to somehow classify what we are seeing in our image. One way to do this is to output another image, classifying each pixel, whether it belongs to the object/s we are looking for. This can be defined as creating a function $f:x \\in \\mathbb R^{W \\times H \\times C} \\to y \\in \\mathbb Z_n$ where $n$ is the number of classes we have in our image. An illustration is provided below. Specifically, this is a case called **semantic segmentation**.\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "   <img src=\"images/semantic_segmentation.jpg\" width=\"750\"/>\n",
    "</div>\n",
    "\n",
    "[Image Source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.hitechbpo.com%2Fblog%2Fsemantic-segmentation-guide.php&psig=AOvVaw2xoq84cC-FovXhPj2KUpDB&ust=1752526997439000&source=images&cd=vfe&opi=89978449&ved=0CBQQjRxqFwoTCNDgz_7duo4DFQAAAAAdAAAAABAE)\n",
    "\n",
    "\n",
    "The case of **contrail segmentation** can then be examined as just a subset of **semantic segmentation$ where our classes are $n=2$ - \"contrail\" and \"background\". Illustration of $x$ and $y$ below:\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "   <img src=\"contrail-seg/data/goes/florida/image/florida_2020_03_05_0101.png\" width=\"600\"/>\n",
    "   <img src=\"contrail-seg/data/goes/florida/mask/florida_2020_03_05_0101.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "[Image Source TBD](TBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ad18e",
   "metadata": {},
   "source": [
    "This is a challenging task. Throughout the study, the authors present ways they try to compensate for their lack of data (only around 30 images). **This is a common occurence in the ML industry**. In the following sections, you will go through exercises examining each of their approaches, try to add to them and apply them to a different case, and reason about if this is a correct application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e7eb29",
   "metadata": {},
   "source": [
    "## How Do We Know Our Model Is Good? - Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a91694",
   "metadata": {},
   "source": [
    "## Can We Train Our Model Better? - Other Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ffbb7f",
   "metadata": {},
   "source": [
    "## Data Scarcity - Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "def train(model, dataloader, epochs=3):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13480c9",
   "metadata": {},
   "source": [
    "#### Natural Language\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add2bff",
   "metadata": {},
   "source": [
    "#### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06932cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.simpleCNN import SimpleCNN\n",
    "from torch.utils.data import Subset\n",
    "import torchvision \n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "def get_balanced_indices_by_targets(targets, samples_per_class):\n",
    "    indices = []\n",
    "    for class_label in range(10):\n",
    "        class_indices = (targets == class_label).nonzero(as_tuple=True)[0]\n",
    "        selected = class_indices[:samples_per_class]\n",
    "        indices.extend(selected.tolist())\n",
    "    return indices\n",
    "\n",
    "transform_no_aug = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "\n",
    "transform_aug = transforms.Compose([\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "subset_size = 100\n",
    "epochs = 50 #WARNING - IF TOO LOW, MAY NOT WORK\n",
    "full_trainset_no_aug = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_no_aug)\n",
    "full_trainset_aug = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_aug)\n",
    "\n",
    "samples_per_class = subset_size // 10\n",
    "balanced_indices = get_balanced_indices_by_targets(full_trainset_no_aug.targets, samples_per_class)\n",
    "\n",
    "trainset_no_aug = Subset(full_trainset_no_aug, balanced_indices)\n",
    "trainset_aug = Subset(full_trainset_aug, balanced_indices)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_no_aug)\n",
    "\n",
    "trainloader_no_aug = torch.utils.data.DataLoader(trainset_no_aug, batch_size=64, shuffle=True)\n",
    "trainloader_aug = torch.utils.data.DataLoader(trainset_aug, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "model_no_aug = SimpleCNN()\n",
    "train(model_no_aug, trainloader_no_aug, epochs=epochs)\n",
    "acc_no_aug = test(model_no_aug, testloader)\n",
    "\n",
    "model_aug = SimpleCNN()\n",
    "train(model_aug, trainloader_aug, epochs=epochs)\n",
    "acc_aug = test(model_aug, testloader)\n",
    "\n",
    "print(f\"Accuracy without augmentation: {acc_no_aug:.4f}\")\n",
    "print(f\"Accuracy with augmentation: {acc_aug:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54715e8b",
   "metadata": {},
   "source": [
    "#### Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65a9a3c",
   "metadata": {},
   "source": [
    "## Can We Take Advantage of Other Tasks - Transfer Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
