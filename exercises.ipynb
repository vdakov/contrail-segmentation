{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7befc7ec",
   "metadata": {},
   "source": [
    "# Case Study: Contrail Segmentation \n",
    "## On the Importance of Understanding Your Data and Task\n",
    "*Rolls off the tongue, doesn't it?* - The Authors of This Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb08c90e",
   "metadata": {},
   "source": [
    "This notebook presents a set of exercises in the form of a case study on segmenting contrails from satellite image (also referred to as remote sensing data). \n",
    "\n",
    "*Prerequisties*:\n",
    "- Basic machine learning knowledge \n",
    "- Basic familiarity with the paper \"Few-Shot Contrail Segmentation in Remote Sensing Imagery With Loss Function in Hough Space\" by Junzi et al. (https://ieeexplore.ieee.org/document/10820969). We do not expect you to understand the whole paper, but rather to have an idea of what is being done.\n",
    "\n",
    "*The goal by the end of this notebook is for students to:*\n",
    "- Gain an exposure to a new application of artificial intelligence - extracing semantics from satellite images \n",
    "- Understand when they can consider the specifics of their data and task\n",
    "- Gain exposure to methods to take advantage of their task\n",
    "- Understand how to evaluate machine learning tasks based on the goal they are solving \n",
    "- Go beyond the technical understanding of the problem and consider where this application will be used, will it even be helpful, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7518a7a3",
   "metadata": {},
   "source": [
    "## Introduction: Contail Segmentation\n",
    "The paper \"Few-Shot Contrail Segmentation in Remote Sensing Imagery With Loss Function in Hough Space\" by Junzi et al. focuses on creating an automatic segmentation procedure for contrails when using few data samples by taking advantage of what we know about the problem. In this section, the problem is introduced from the very basics of what a contrails is, to a mathematical formulation of the issue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc898b0f",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images/what-is-a-contrail.png\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "*What is a contrail?* \n",
    "\n",
    "Contrails or vapour trails are line-shaped clouds produced by aircraft engine exhaust or changes in air pressure, typically at aircraft cruising altitudes several kilometres/miles above the Earth's surface. (Definition - https://en.wikipedia.org/wiki/Contrail)\n",
    "\n",
    "*Why does it matter?* \n",
    "\n",
    "The Sun emits solar radiation towards the Earth that the ground traditionally reflects back. However, the formation of the ice crystals in contrails creates a dense enough \"shield\" to reflect a part of them back. This impacts the amount of radiation trapped around the Earth and thus contributes to the temperature. They also have a potential cooling effect on sun rays getting reflected back towards the Sun, though this effect is currently estimated to be smaller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31becf5",
   "metadata": {},
   "source": [
    "**Satellite Image** \n",
    "\n",
    "Satellite images are a unique form of data. As the name implies, they are taken by a satellite, which leads to charactersitics about it, such as:\n",
    "- **top-down**: the perspective of the image is always orthogonal towards the ground \n",
    "- **distance from the Earth**: Depending on the image, and satellite both the distance to the Earth and the resolution of the image is different. \n",
    "    - For this reason, the resolution is measured in terms of actual distance (e.g. A resolution of 30cm means that 30cm of information is encoded per pixel.)\n",
    "    - Common resolutions are 30cm, 1m, 2m. Correspondingly a smaller distance corresponds to higher quality, more expense, harder to get, etc.\n",
    "- **multispectral data**: Sensory information from satellites is more advanced than traditional cameras. They can capture more than just the color spectrum. Each one is a special band (channel in images). Below are listed some examples: \n",
    "    - RGB (Red-Green-Blue): The channels traditional cameras capture, and are blended together to create a final image\n",
    "    - Infrared/Near-Infrared: A channel where some surfaces react differently to - for example, vegetation reacts differently to infrared light. Can reveal new information, not visible to the naked eye.\n",
    "    - Point Clouds (LiDAR): A channel capturing distance to the earth. Used to map out things like elevation via the laser reflecting back. \n",
    "    - And many others, depending on the sensor used ... We will introduce them in this notebook if and as necessary.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "   <img src=\"images/remote-sensing-platforms.webp\" width=\"450\"/>\n",
    "\n",
    "</div>\n",
    "\n",
    "[Image Source: GeeksForGeeks](https://imgs.search.brave.com/ebFlnbhNCKYFr760b0JRrC-BQlpJUeTmlB5SUw-5Nf4/rs:fit:860:0:0:0/g:ce/aHR0cHM6Ly9tZWRp/YS5nZWVrc2ZvcmdZWtzLm9yZy93cC)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Below are listed some example images of contrail images taken from different satellites and bands: \n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "  <img src=\"images/1-MeteoSat 11.png\" width=\"350\"/>\n",
    "  <img src=\"images/2-NASA Terra.jpg\" width=\"350\"/>\n",
    "  <img src=\"images/3-NOAA Suomi-NPP.jpg\" width=\"350\"/>\n",
    "</div>\n",
    "\n",
    "[Image Source TBD](TBD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a02d044",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab45ba0",
   "metadata": {},
   "source": [
    "### Create an Anaconda Environment and Download Requirements\n",
    "If you haven't already setup the codebase following the instructions of the README, you can do so here. Otherwise, you can skip running this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%conda create -y -n contrail-project python=3.12\n",
    "%conda activate contrail-project\n",
    "%conda install -y pip\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55189639",
   "metadata": {},
   "source": [
    "### Get Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "    # Force deterministic behavior in cudnn (might slow things down)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271b870",
   "metadata": {},
   "source": [
    "## What is Contrail Segmentation?\n",
    "\n",
    "What are we trying to accomplish? From the name **contrail segmentation**, and the information beforehand, we know that we are trying to extract information out of the satellite images. To do so, we need to define **segmentation** first.\n",
    "\n",
    "#### Segmentation\n",
    "\n",
    "Recall **classification**, abstracted from any machine learning model. Given an input $x$, we want to get an output $\\widehat y$ corresponding to the correct class $y$ of the object $x$. \n",
    "\n",
    "In remote sensing, our inputs are images. Therefore $x \\in \\mathbb R^{W \\times H \\times C} $ where $W, H$ are the height and width (in pixels) of the current image and $C$ the number of channels total the image (recall RGB or Infrared introduced previously). Mathematically, this is a **tensor** (it is not very important for today, but good to know in general). \n",
    "\n",
    "In **segmentation**, we want to semantically understand our image. A way to do this is to somehow classify what we are seeing in our image. One way to do this is to output another image, classifying each pixel, whether it belongs to the object/s we are looking for. This can be defined as creating a function $f:x \\in \\mathbb R^{W \\times H \\times C} \\to y \\in \\mathbb Z_n$ where $n$ is the number of classes we have in our image. An illustration is provided below. Specifically, this is a case called **semantic segmentation**.\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "   <img src=\"images/semantic_segmentation.jpg\" width=\"750\"/>\n",
    "</div>\n",
    "\n",
    "[Image Source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.hitechbpo.com%2Fblog%2Fsemantic-segmentation-guide.php&psig=AOvVaw2xoq84cC-FovXhPj2KUpDB&ust=1752526997439000&source=images&cd=vfe&opi=89978449&ved=0CBQQjRxqFwoTCNDgz_7duo4DFQAAAAAdAAAAABAE)\n",
    "\n",
    "\n",
    "The case of **contrail segmentation** can then be examined as just a subset of **semantic segmentation** where our classes are $n=2$ - \"contrail\" and \"background\". Illustration of $x$ and $y$ below:\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "   <img src=\"contrail-seg/data/goes/florida/image/florida_2020_03_05_0101.png\" width=\"600\"/>\n",
    "   <img src=\"contrail-seg/data/goes/florida/mask/florida_2020_03_05_0101.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "[Image Source TBD](TBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ad18e",
   "metadata": {},
   "source": [
    "This is a challenging task. Throughout the study, the authors present ways they try to compensate for their lack of data (only around 30 images). **This is a common occurence in the ML industry**. In the following sections, you will go through exercises examining each of their approaches, try to add to them and apply them to a different case, and reason about if this is a correct application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e7eb29",
   "metadata": {},
   "source": [
    "## How Do We Know Our Model Is Good? - Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a91694",
   "metadata": {},
   "source": [
    "## Can We Train Our Model Better? - Other Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ffbb7f",
   "metadata": {},
   "source": [
    "## Data Scarcity - Data Augmentation\n",
    "\n",
    "Scarcity is an issue everybody who works with data faces eventually, even in foundational work - https://www.eecis.udel.edu/~shatkay/Course/papers/NetworksAndCNNClasifiersIntroVapnik95.pdf. **Data Augmentation** is a way to reduce overfitting on incomplete datasets. \n",
    "\n",
    "Intuitively, it introduces distortions to the small data samples, which vary during training so that the machine learning model of choice does not focus on arbitrary aspects of the data (such as the position of the object, such as a specific position or number). \n",
    "\n",
    "Mathematically, it changes the sampling from the training data set $X_{train}$ by a  affine transformation function $g: \\mathbb R^n \\to \\mathbb R^n $, which is applied on every sample point $x \\in X$ and outputs a different transformation each time. This change effetively increases the diversity in the training set distribution and brings the final estimate closer to the maximum likelihood on the complete data set $X$ where $X_{train} \\subseteq X$. \n",
    "\n",
    "*Example* \n",
    "\n",
    " Imagine a scenario where we have a small training set on flowers (such as the Iris dataset), where all daisies and poppies in the training set have a sepal width $s$ of $5$ and $7$ centimeters,  respectively. The test set, and correspondingly all real examples may have no such examples and there are daisies and poppies of many more sizes. A data augmentations technique may be making $g$ apply noise to the inputs, putting the daisies and poppies's $s$ within some standard deviation giving a random variable over the dataset $$s_{\\text{augmented}} = s + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal N(0, \\sigma^2)$$\n",
    "\n",
    " For instance, daisies might be sampled with sepal widths $5 \\pm 0.5$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f1a86",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "# Securely input your token\n",
    "token = getpass('Paste your GitLab token: ')\n",
    "\n",
    "# Set the repository URL\n",
    "repo_url = \"https://oauth2:\" + token + \"@gitlab.ewi.tudelft.nl/tjviering/contrails.git\"\n",
    "\n",
    "# Clone the repository\n",
    "!git clone --branch data-augmentation --single-branch {repo_url}\n",
    "# 9WrCsxUVdPYmy7uij2Fn -Token\n",
    "# Verify clone\n",
    "!ls contrails  # Should show repository contents\n",
    "os.chdir(\"contrails\")\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3249c8ee",
   "metadata": {},
   "source": [
    "**This section introduces data augmentation in two modalities: audio and images. Later, we will also show the value of that on our task of contrails.**\n",
    "\n",
    "One is a fully-guided example, the other ones need to have either code filled in and/or theoretical questions answered. Finally, this is all related to the contrail case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54715e8b",
   "metadata": {},
   "source": [
    "### Audio\n",
    "\n",
    "Here we show a simple task: audio classification. Given an audio file, can we understand automatically what number is being said based on the recording? For this purpose, we are going to use a subset of the AudioMNIST dataset, and evaluate performance on it both with and without data augmentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7d0f92",
   "metadata": {},
   "source": [
    "https://github.com/soerenab/AudioMNIST - AudioMNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0b310",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cfb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from data_util.AudioMNIST.load_data import load_audio_data, load_audio_data_noisy\n",
    "\n",
    "max_samples = 100\n",
    "X_audio, y_audio, wavs = load_audio_data('data_util/AudioMNIST/free-spoken-digit-dataset-master/recordings', max_samples=max_samples)\n",
    "X_audio_augmented, y_audio_augmented, wavs_augmented = load_audio_data_noisy('data_util/AudioMNIST/free-spoken-digit-dataset-master/recordings', max_samples=max_samples)\n",
    "\n",
    "X_audio_train, X_audio_test, y_audio_train, y_audio_test = train_test_split(X_audio, y_audio, test_size=0.2, random_state=42)\n",
    "X_audio_train_augmented, _, y_audio_train_augmented, _ = train_test_split(X_audio_augmented, y_audio_augmented, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb3dad",
   "metadata": {},
   "source": [
    "#### Original Audio Samples\n",
    "\n",
    "An audio file is created when an analog processor converts sound waves into a sequence of amplitudes and spectrograms. They respectively give an idea of when the volume and frequency are higher. When it is played, it is also just an array of amplitudes, showing the intensity. By looking at the waveform graphs, you can clearly discern when someone is speaking, and also play the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.show_audio import visualize_waveform_and_play_audio\n",
    "visualize_waveform_and_play_audio(y_audio[:5], wavs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba59e3",
   "metadata": {},
   "source": [
    "#### Augmented Audio Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e19fa2",
   "metadata": {},
   "source": [
    "This signal seems to have lots of amplitudes, right, and varying length, based on the person, voice, etc. We have prepared an MNIST dataset, of numbers. However, if we change certain things about the signal, would it necessarily change the label. For example:\n",
    "- speeding up or slowing down the speech still gives the same number \n",
    "- shifting the pitch as well\n",
    "- adding noise, making a worse recording should not change the outcome as well\n",
    "\n",
    "This is all a form of **data augmentation** - changes to our data that does not impact how our model should treat it, while being more adversarial and challenging for it. Below we have prepared one of the examples previously listed - adding Gaussian noise. Take a listen and see if the numbers are still discernable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c14bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_waveform_and_play_audio(y_audio_augmented[:5], wavs_augmented[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37885fde",
   "metadata": {},
   "source": [
    "#### Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca602fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_util.AudioMNIST.split_data import get_balanced_indices_by_targets\n",
    "\n",
    "subset_size = 500\n",
    "samples_per_class = subset_size // 10\n",
    "\n",
    "balanced_indices = get_balanced_indices_by_targets(y_audio_train, samples_per_class, n_classes=10)\n",
    "X_audio_train_subset = X_audio_train[balanced_indices]\n",
    "y_audio_train_subset = y_audio_train[balanced_indices]\n",
    "balanced_indices = get_balanced_indices_by_targets(y_audio_train_augmented, samples_per_class, n_classes=10)\n",
    "X_audio_train_augmented_subset = X_audio_train_augmented[balanced_indices]\n",
    "y_audio_train_augmented_subset = y_audio_train_augmented[balanced_indices]\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model_no_data_aug = GradientBoostingClassifier()\n",
    "model_no_data_aug.fit(X_audio_train_subset, y_audio_train_subset)\n",
    "acc_no_data_aug = model_no_data_aug.score(X_audio_test, y_audio_test)\n",
    "print(f\"Accuracy without data augmentation: {acc_no_data_aug:.4f}\")\n",
    "\n",
    "model_data_aug = GradientBoostingClassifier()\n",
    "model_data_aug.fit(X_audio_train_augmented_subset, y_audio_train_augmented_subset)\n",
    "acc_data_aug = model_data_aug.score(X_audio_test, y_audio_test)\n",
    "print(f\"Accuracy with data augmentation: {acc_data_aug:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648616a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from visualization.show_metrics import plot_accuracy_vs_subset_size\n",
    "\n",
    "subset_sizes = [100, 200, 300, 400, 500, 700, 1000]  # Different subset sizes to try\n",
    "acc_no_aug_list = []\n",
    "acc_aug_list = []\n",
    "\n",
    "for subset_size in subset_sizes:\n",
    "    samples_per_class = subset_size // 10\n",
    "    \n",
    "    # Get balanced subsets (no augmentation)\n",
    "    balanced_indices = get_balanced_indices_by_targets(y_audio_train, samples_per_class, n_classes=10)\n",
    "    X_train_sub = X_audio_train[balanced_indices]\n",
    "    y_train_sub = y_audio_train[balanced_indices]\n",
    "    \n",
    "    # Get balanced subsets (with augmentation)\n",
    "    balanced_indices_aug = get_balanced_indices_by_targets(y_audio_train_augmented, samples_per_class, n_classes=10)\n",
    "    X_train_aug_sub = X_audio_train_augmented[balanced_indices_aug]\n",
    "    y_train_aug_sub = y_audio_train_augmented[balanced_indices_aug]\n",
    "    \n",
    "    # Train and evaluate model without augmentation\n",
    "    model_no_aug = GradientBoostingClassifier()\n",
    "    model_no_aug.fit(X_train_sub, y_train_sub)\n",
    "    acc_no_aug = model_no_aug.score(X_audio_test, y_audio_test)\n",
    "    acc_no_aug_list.append(acc_no_aug)\n",
    "    \n",
    "    # Train and evaluate model with augmentation\n",
    "    model_aug = GradientBoostingClassifier()\n",
    "    model_aug.fit(X_train_aug_sub, y_train_aug_sub)\n",
    "    acc_aug = model_aug.score(X_audio_test, y_audio_test)\n",
    "    acc_aug_list.append(acc_aug)\n",
    "\n",
    "plot_accuracy_vs_subset_size(\n",
    "    subset_sizes=subset_sizes,\n",
    "    acc_no_aug_list=acc_no_aug_list,\n",
    "    acc_with_aug_list=acc_aug_list,\n",
    "    title='Performance: Impact of Data Augmentation vs. Subset Size',\n",
    "    save_path='accuracy_comparison.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b812110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "# First tree - no augmentation\n",
    "plot_tree(model_no_data_aug.estimators_[0, 0], filled=True,\n",
    "          feature_names=[f\"feat_{i}\" for i in range(X_audio_train_subset.shape[1])],\n",
    "          ax=axs[0])\n",
    "axs[0].set_title(\"First Decision Tree - No Data Augmentation\")\n",
    "\n",
    "# First tree - with augmentation\n",
    "plot_tree(model_data_aug.estimators_[0, 0], filled=True,\n",
    "          feature_names=[f\"feat_{i}\" for i in range(X_audio_train_augmented_subset.shape[1])],\n",
    "          ax=axs[1])\n",
    "axs[1].set_title(\"First Decision Tree - With Data Augmentation\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fbf09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "\n",
    "accuracies = [acc_no_data_aug, acc_data_aug]\n",
    "labels = ['No Data Augmentation', 'With Data Augmentation']\n",
    "\n",
    "axs[0].bar(labels, accuracies, color=['red', 'green'])\n",
    "axs[0].set_ylim([0, 1])\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_title('Model Accuracy Comparison')\n",
    "for i, acc in enumerate(accuracies):\n",
    "    axs[0].text(i, acc + 0.02, f\"{acc:.4f}\", ha='center')\n",
    "    \n",
    "fi_no_aug = model_no_data_aug.feature_importances_\n",
    "fi_aug = model_data_aug.feature_importances_\n",
    "\n",
    "indices = np.arange(len(fi_no_aug))\n",
    "\n",
    "axs[1].bar(indices - 0.2, fi_no_aug, width=0.4, label='No Augmentation')\n",
    "axs[1].bar(indices + 0.2, fi_aug, width=0.4, label='With Augmentation')\n",
    "axs[1].set_xlabel('Feature Index')\n",
    "axs[1].set_ylabel('Feature Importance')\n",
    "axs[1].set_title('Feature Importances Comparison')\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add2bff",
   "metadata": {},
   "source": [
    "#### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "def train(model, dataloader, epochs=3):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for images, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06932cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_util.MNIST.raw.mnist_loading import load_mnist_data\n",
    "from models.simpleCNN import SimpleCNN\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from visualization.show_images import visualize_pytorch_dataset_images\n",
    "import albumentations as A\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class AlbumentationsTransform:\n",
    "    def __init__(self, aug):\n",
    "        self.aug = aug\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # torchvision datasets give PIL.Image, convert to numpy\n",
    "        img = np.array(img)\n",
    "        augmented = self.aug(image=img)\n",
    "        img = augmented[\"image\"]\n",
    "        # convert back to tensor\n",
    "        if isinstance(img, np.ndarray):\n",
    "            img = torch.from_numpy(img).float()\n",
    "            if img.ndim == 2:  # add channel dim if grayscale\n",
    "                img = img.unsqueeze(0)\n",
    "            elif img.ndim == 3 and img.shape[2] in [1,3]:\n",
    "                img = img.permute(2,0,1)  # HWC -> CHW\n",
    "        return img\n",
    "\n",
    "\n",
    "\n",
    "transform_no_aug = AlbumentationsTransform(A.Compose([\n",
    "    A.Normalize(mean=0.5, std=0.5, max_pixel_value=255.0),\n",
    "]))\n",
    "\n",
    "\n",
    "transform_aug = AlbumentationsTransform(\n",
    "    A.Compose([\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "        A.Affine(translate_percent=(0.1, 0.1), p=0.5),\n",
    "        A.GaussianBlur(blur_limit=(3, 5), p=0.5, sigma_limit=(0.1, 2.0)),\n",
    "        A.Normalize(mean=0.5, std=0.5, max_pixel_value=255.0),\n",
    "    ])\n",
    ")\n",
    "\n",
    "subset_size = 100\n",
    "num_classes = 10\n",
    "epochs = 100 #WARNING - IF TOO LOW, MAY NOT WORK\n",
    "samples_per_class = subset_size // 10\n",
    "\n",
    "trainset_no_aug, trainloader_no_aug, testloader = load_mnist_data(train=True,data_augmentations=transform_no_aug,  subset_size=subset_size, )\n",
    "trainset_aug, trainloader_aug, _ = load_mnist_data(train=True, data_augmentations=transform_aug, subset_size=subset_size)\n",
    "\n",
    "\n",
    "visualize_pytorch_dataset_images(trainset_no_aug, \"No Data Augmentation\")\n",
    "visualize_pytorch_dataset_images(trainset_aug, \"With Data Augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70743095",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_aug = SimpleCNN()\n",
    "losses_no_aug = train(model_no_aug, trainloader_no_aug, epochs=epochs)\n",
    "acc_no_aug = test(model_no_aug, testloader)\n",
    "\n",
    "model_aug = SimpleCNN()\n",
    "losses_aug = train(model_aug, trainloader_aug, epochs=epochs)\n",
    "acc_aug = test(model_aug, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b59fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.show_metrics import compare_two_losses\n",
    "\n",
    "print(f\"Accuracy without augmentation: {acc_no_aug:.4f}\")\n",
    "print(f\"Accuracy with augmentation: {acc_aug:.4f}\")\n",
    "\n",
    "compare_two_losses(losses_no_aug, 'No Data Augmentation', losses_aug, \"With Data Augmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac0746",
   "metadata": {},
   "source": [
    "### Contrail Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"contrail-seg\")\n",
    "from train import train_contrail_network\n",
    "\n",
    "transform_aug = A.Compose([\n",
    "    A.Rotate(limit=15, p=0.5),\n",
    "    A.Affine(translate_percent=(0.1, 0.1), p=0.5),\n",
    "    A.Resize(128, 128),\n",
    "    A.Normalize(mean=0.5, std=0.5, max_pixel_value=255.0),\n",
    "])\n",
    "\n",
    "\n",
    "train_contrail_network(transform_aug, 100, \"dice\", \"base\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
