{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed6c01ee",
   "metadata": {},
   "source": [
    "# Case Study: Contrail Segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96d941",
   "metadata": {},
   "source": [
    "This notebook presents a set of exercises in the form of a case study on segmenting contrails from satellite image (also referred to as remote sensing data). In it you will be introduced to a data science scenario you will have to tackle, challenge the issue of data scarcity, talk about the ethics of the usage of your model, and how to verify your model works on a technical level.\n",
    "\n",
    "*Prerequisties*:\n",
    "- Basic machine learning knowledge \n",
    "\n",
    "*The goal by the end of this notebook is for students to:*\n",
    "- Gain an exposure to a new application of artificial intelligence - extracing semantics from satellite images.\n",
    "- Understand when they can consider the specifics of their data and task.\n",
    "- Think about the implications of their machine learning beyound the technical - what will be my machine learning be used for? \n",
    "- Understand how to evaluate machine learning tasks based on the goal they are solving - how more complicated tasks need more complicated metrics.\n",
    "- Become more informed as engineers in their place in the whole machine learning pipeline.\n",
    "\n",
    "*The notebook is loosely based on the paper \"Few-Shot Contrail Segmentation in Remote Sensing Imagery With Loss Function in Hough Space\" by Junzi et al. It focuses on creating an automatic segmentation procedure for contrails when using few data samples by taking advantage of what we know about the problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90b9ad4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  What is a contrail?\n",
    "\n",
    "Contrails are line-shaped clouds produced by aircraft engine exhaust due to changes in air pressure, typically at aircraft cruising altitudes several kilometres/miles above the Earth's surface. (Definition - https://en.wikipedia.org/wiki/Contrail)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://raw.githubusercontent.com/vdakov/contrail-segmentation/main/images/what-is-a-contrail.png\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "[Image Source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fsites.research.google%2Fgr%2Fcontrails%2F&psig=AOvVaw128OqAWqIAmXihrXxVq9OS&ust=1763826829380000&source=images&cd=vfe&opi=89978449&ved=0CBEQjRxqFwoTCJjx8JHNg5EDFQAAAAAdAAAAABAE)\n",
    "\n",
    "## Why does it matter?\n",
    "\n",
    "The Sun emits solar radiation towards the Earth that the ground traditionally reflects back. However, the formation of the ice crystals in contrails creates a dense enough \"shield\" to reflect a part of them back. This impacts the amount of radiation trapped around the Earth and thus contributes to the temperature. They also have a potential cooling effect on sun rays getting reflected back towards the Sun, though this effect is currently estimated to be smaller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3afd2",
   "metadata": {},
   "source": [
    "## Problem Definition\n",
    "\n",
    "The contrails reflect light and cool during the day, but trap heat during the night. This becomes an issue when contrails stick around for hours in specific regions – ISSRs – 80% of contrail warming  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ae188b",
   "metadata": {},
   "source": [
    "## The Challenge\n",
    "\n",
    "Contrails are already being measured by aircraft sensors and estimation models – as the ones responsible if something goes wrong. In the future, however, with issues like global warming, governments may want to do contrail reporting themselves. While there are already ways this being  done via sensors and other models, they are provided by the airlines. The government needs an independent version of these reports."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c09e72d",
   "metadata": {},
   "source": [
    "## Can we solve this with AI?\n",
    "\n",
    "Yes, using satellite images and neural networks, we can make a model to segment the contrails from arbitrary images. Good news: using satellite \n",
    "images and neural networks, we can make a model to segment the contrails from arbitrary images \n",
    "\n",
    "###  What is Contrail Segmentation?\n",
    "\n",
    "Contrail segmentation is a type of semantic segmentation, where the goal is to identify which parts of a satellite image contain contrails.\n",
    "\n",
    "### Segmentation \n",
    "\n",
    "In normal image classification, you assign a single label to an entire image.\n",
    "In segmentation, you label every pixel so the model can understand what objects appear and where they are.\n",
    "\n",
    "### Contrail Segmentation\n",
    "\n",
    "For contrail segmentation, there are only two pixel types:\n",
    "\n",
    "- pixels that are part of a contrail\n",
    "- pixels that are background\n",
    "\n",
    "The model takes a satellite image and produces another image (a mask) showing where contrails are located.\n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "   \n",
    "   <img src=\"https://raw.githubusercontent.com/vdakov/contrail-segmentation/main/contrail-seg/data/goes/florida/image/florida_2020_03_05_0101.png\" width=\"400\"/>\n",
    "   <img src=\"https://raw.githubusercontent.com/vdakov/contrail-segmentation/main/contrail-seg/data/goes/florida/mask/florida_2020_03_05_0101.png\" width=\"400\">\n",
    "\n",
    "</div>\n",
    "\n",
    "[Image Source](https://www.kaggle.com/competitions/google-research-identify-contrails-reduce-global-warming/data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df04c4bb",
   "metadata": {},
   "source": [
    "Below are listed some example images of contrail images taken from different satellites and bands: \n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/vdakov/contrail-segmentation/main/images/1-MeteoSat 11.png\" height=\"350\" width=\"350\"/>\n",
    "  <img src=\"https://raw.githubusercontent.com/vdakov/contrail-segmentation/main/images/2-NASA Terra.jpg\" height=\"350\" width=\"350\"/>\n",
    "  <img src=\"https://raw.githubusercontent.com/vdakov/contrail-segmentation/main/images/3-NOAA Suomi-NPP.jpg\" height=\"350\" width=\"350\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d792f8b8",
   "metadata": {},
   "source": [
    "## The Scenario \n",
    "You are a data scientist at VeryCoolCompany Ltd. And the government has asked you to try to make this for them.  In this notebook you will tackle four challenges in regards to this: \n",
    "\n",
    "- **Data issues**: data is expensive, hard to get, label, etc. It costs money for your company. Sometimes even your datasets are samey. Different types of contrails \n",
    "- **Scientific Uncertainty**: Uncertainty in CO2 forcing, up to 70% of the actual impact on contrails\n",
    "- **Model Quality and Uncertainty**\n",
    "- **Policy Issues**: How should the government employ this tool you provide them?  Even if a policy is decided, how can it be enforced – globally, locally?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c745b26b",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e61ba",
   "metadata": {},
   "source": [
    "### IF LOCAL: Create an Anaconda Environment and Download Requirements\n",
    "If you haven't already setup the codebase following the instructions of the README, you can do so here. Otherwise, you can skip running this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda create -y -n contrail-project python=3.12\n",
    "# %conda activate contrail-project\n",
    "# %conda install -y pip\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76573823",
   "metadata": {},
   "source": [
    "### IF ON COLAB: Download Repository \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009fbbed",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def is_colab():\n",
    "    return \"COLAB_GPU\" in os.environ or \"google.colab\" in str(get_ipython())\n",
    "\n",
    "def is_kaggle():\n",
    "    return os.path.exists(\"/kaggle\")\n",
    "\n",
    "repo_url = \"https://github.com/vdakov/contrail-segmentation.git\"\n",
    "\n",
    "if is_colab():\n",
    "    # Colab-specific setup\n",
    "\n",
    "    \n",
    "    !git clone --branch dev --single-branch {repo_url}\n",
    "    !ls contrails\n",
    "    \n",
    "    os.chdir(\"contrails\")\n",
    "    !pip install --upgrade pip\n",
    "    !pip install lightning segmentation_models_pytorch\n",
    "    !pip install -r requirements.txt\n",
    "    \n",
    "    os.chdir(\"data_util/AudioMNIST\")\n",
    "    !git clone https://github.com/Jakobovski/free-spoken-digit-dataset.git\n",
    "    os.chdir(\"../../\")\n",
    "\n",
    "elif is_kaggle():\n",
    "    # Kaggle-specific setup\n",
    "    print(\"Running on Kaggle. Installing requirements...\")\n",
    "    \n",
    "    os.chdir(\"/kaggle/working\")  # Ensure working directory\n",
    "    \n",
    "    !git clone --branch dev --single-branch {repo_url}\n",
    "    !ls contrails\n",
    "    \n",
    "    os.chdir(\"contrails\")\n",
    "    !pip install lightning segmentation_models_pytorch ipyml\n",
    "    \n",
    "    \n",
    "    os.chdir(\"data_util/AudioMNIST\")\n",
    "    !git clone https://github.com/Jakobovski/free-spoken-digit-dataset.git\n",
    "    os.chdir(\"../../\")\n",
    "\n",
    "else:\n",
    "    print(\"Not running on Colab or Kaggle. Skipping installation steps.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d385d6",
   "metadata": {},
   "source": [
    "### Get Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27335714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from data_util.AudioMNIST.load_data import load_audio_data, load_audio_data_noisy\n",
    "from visualization.show_audio import visualize_waveform_and_play_audio\n",
    "from data_util.AudioMNIST.split_data import get_balanced_indices_by_targets\n",
    "from visualization.show_metrics import plot_accuracy_vs_subset_size\n",
    "from torch import nn, optim\n",
    "from tqdm import tqdm\n",
    "from data_util.MNIST.raw.mnist_loading import load_mnist_data\n",
    "from models.simpleCNN import SimpleCNN\n",
    "from visualization.show_images import visualize_pytorch_dataset_images\n",
    "import albumentations as A\n",
    "from visualization.interactive_menu import retrieve_mnist_menu, AlbumentationsTransform\n",
    "import sys\n",
    "import cv2\n",
    "sys.path.append(\"contrail-seg\")\n",
    "from train import train_contrail_network\n",
    "from evaluate import evaluate_contrail_model, plot_pr_curve\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd8cb1",
   "metadata": {},
   "source": [
    "## Which Dataset to Choose? \n",
    "\n",
    "You are a data scientist. All of your issues start at the dataset you will use for the task at hand. Wherever you go, you first need to consider your domain. In this case, the domain is satellite images and flight trajectories. You need to consider which satellite to collect images from. \n",
    "\n",
    "<!-- <iframe src=\"https://drive.google.com/file/d/1J7-wbA8b5QWF_o-rdsUfArvNTpirGLac/preview\" width=\"1000\" height=\"600\" allow=\"autoplay\"></iframe> -->\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/vdakov/contrail-segmentation/dev/images/landsat-vs-goes.png\" width= \"1000\">\n",
    "\n",
    "\n",
    "<!-- https://drive.google.com/file/d/1J7-wbA8b5QWF_o-rdsUfArvNTpirGLac/view?usp=drive_link) -->\n",
    "\n",
    "[Image Source 1](https://www.kaggle.com/competitions/google-research-identify-contrails-reduce-global-warming/data)\n",
    "[Image Source 2](https://console.cloud.google.com/storage/browser/landsat_contrails_dataset)\n",
    "\n",
    "\n",
    "We present you with two options: LandSAT and GEOS. In the following section, you have to answer the following overarching question, and then a series of questions: \n",
    "\n",
    "#### Does it matter what satellite we pick? \n",
    "\n",
    "Base your answer on this list of characteristics, as well as the resources provided below:\n",
    "**LandSat Characteristics**\n",
    "- Low Earth Orbit \n",
    "- Orbits once every 16 days \n",
    "- 8 channels \n",
    "- Lower resolution\n",
    "For more, please consult https://landsat.gsfc.nasa.gov/satellites/landsat-8/. \n",
    "\n",
    "**GEOS Characteristics**  \n",
    "- Higher Orbit \n",
    "- Higher Resolution \n",
    "- Continuous monitoring over an area\n",
    "\n",
    "For more, please consult https://science.nasa.gov/mission/goes/. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd138f0",
   "metadata": {},
   "source": [
    "<div style=\" font-family: Arial, sans-serif; background: #00e5ff; color: #0b1220; padding: 15px 20px; border-radius: 12px; line-height: 1.6; max-width: 1000px; margin-bottom: 20px; \"> \n",
    "\n",
    "**Question 1:** What are the tradeoffs between the two datasets?\n",
    "\n",
    "- To understand this, look up the datasets online: (Landsat: https://landsat.gsfc.nasa.gov/satellites/landsat-8/; GOES: https://science.nasa.gov/mission/goes/.)\n",
    ")\n",
    "- How did the data collection differ? \n",
    "- What variations (technical differences) do the different datasets have?\n",
    "- Could there be any bias in either dataset? Why?\n",
    "- In the context of the VeryCoolCompany; do you see any issues with either dataset?  \n",
    "--- \n",
    "\n",
    "**Answer:**\n",
    "\n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER --> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe3b6c2",
   "metadata": {},
   "source": [
    "# Data Scarcity - Data Augmentation\n",
    "\n",
    "Scarcity is an issue everybody who works with data faces eventually. Data is expensive to acquire, hard to label, and a good machine learning model benefits from as much .(good) data as possible. \n",
    "\n",
    "Let's take the Google GOES dataset for example. Details for it are provided in https://www.kaggle.com/competitions/google-research-identify-contrails-reduce-global-warming/data. How much do you think it costs?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a75ee4",
   "metadata": {},
   "source": [
    "<div style=\" font-family: Arial, sans-serif; background: #00e5ff; color: #0b1220; padding: 15px 20px; border-radius: 12px; line-height: 1.6; max-width: 1000px; margin-bottom: 20px; \"> \n",
    "\n",
    "\n",
    "**Question 2:** Open the dataset link above. Examine the dataset and try to estimate its costs. Think about the number of images, how they get labeled, and how much that might cost. It's fine if you make assumptions - this is not a financial analysis.\n",
    "\n",
    "--- \n",
    "\n",
    "**Answer:**\n",
    "\n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f515230",
   "metadata": {},
   "source": [
    "Your boss at VeryCoolCompany saw this cost and got horrified at it! This is more than the project would cost otherwise. As such, we will use a very small dataset, hand-labeled dataset of Florida and San Francisco (about 40 images total). You have to find a way to make the contrail detection task work with very little data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef2974",
   "metadata": {},
   "source": [
    "\n",
    "**Data Augmentation** is a way to reduce overfitting on small datasets, and tackle all of this issue. \n",
    "\n",
    "It is a set of transformations on the data samples that do not change its labels. The transformations change depending on the data being modified - every task has different semantics. The idea is to make the models focus on the essense of the data, rather than arbitrary aspects of it (noise). These transformations may very during training.\n",
    "\n",
    "**This section introduces data augmentation in two modalities: audio and images. Later, we will also show the value of that on our task of contrails.**\n",
    "\n",
    "To illustrate the point of data augmentation, we will guide you through two examples: on an audio and on an image data set. One is a fully-guided example, the other ones need to have either code filled in and/or theoretical questions answered. Finally, this is all related to the contrail case. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1584b2f5",
   "metadata": {},
   "source": [
    "### Audio\n",
    "\n",
    "Here we show a simple task: audio classification. Given an audio file, can we understand automatically what number is being said based on the recording? For this purpose, we are going to use a subset of the FSDD dataset, and evaluate performance on it both with and without data augmentation. The dataset contains recordings of spoken numbers, similar to MNIS but for audio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdf843c",
   "metadata": {},
   "source": [
    "Free Spoken Digit Dataset (FSDD)- https://github.com/Jakobovski/free-spoken-digit-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafff2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 100\n",
    "X_audio, y_audio, wavs = load_audio_data('data_util/AudioMNIST/free-spoken-digit-dataset/recordings', max_samples=max_samples)\n",
    "X_audio_train, X_audio_test, y_audio_train, y_audio_test = train_test_split(X_audio, y_audio, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab5a12a",
   "metadata": {},
   "source": [
    "#### Original Audio Samples\n",
    "\n",
    "An audio file is created when an analog processor converts sound waves into a sequence of amplitudes and spectrograms. They respectively give an idea of when the volume and frequency are higher. When it is played, it is also just an array of amplitudes, showing the intensity. By looking at the waveform graphs, you can clearly discern when someone is speaking, and also play the audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_waveform_and_play_audio(y_audio[:3], wavs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bad6f5",
   "metadata": {},
   "source": [
    "#### Augmented Audio Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a16b10b",
   "metadata": {},
   "source": [
    "This signal seems to have lots of amplitudes, right, and varying length, based on the person, voice, etc. We have prepared an MNIST dataset, of numbers. It turns out that there are transformations we can do that will not change the label - in other words, the semantics are preserved. For example:\n",
    "- speeding up or slowing down the speech still gives the same number \n",
    "- shifting the pitch as well\n",
    "- adding noise, making a worse recording should not change the outcome as well\n",
    "\n",
    "This is all a form of **data augmentation** - changes to our data that does not impact how our model should treat it, while being more adversarial and challenging for it. Below we have prepared one of the examples previously listed - adding Gaussian noise. \n",
    "\n",
    "Below is a cell performing all of these perturbations as a function. We have already pre-loaded it for you on the same dataset.\n",
    "\n",
    "Take a listen and see if the numbers are still discernable. Also, try to examine how the signal looks visually different. Is there anything different about the wav file and the spectrograms? Take a look and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b72a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2, random_state=None, shuffle=True):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    indices = np.arange(n_samples)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    test_count = int(n_samples * test_size)\n",
    "    \n",
    "    test_indices = indices[:test_count]\n",
    "    train_indices = indices[test_count:]\n",
    "    \n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def augmentation_function(x, pitch_scale_min=0.9, pitch_scale_max=1.1, slow_down_factor=0.85, sigma=0.001):\n",
    "    import numpy as np\n",
    "    x = x * np.random.uniform(pitch_scale_min, pitch_scale_max) \n",
    "    x = np.interp(np.arange(0, len(x), slow_down_factor), np.arange(0, len(x)), x)\n",
    "    x = x + np.random.normal(0, sigma, len(x))  \n",
    "    return x\n",
    "\n",
    "X_audio_augmented, y_audio_augmented, wavs_augmented = load_audio_data_noisy('data_util/AudioMNIST/free-spoken-digit-dataset/recordings', augmentation_function, max_samples=max_samples)\n",
    "X_audio_train_augmented, _, y_audio_train_augmented, _ = train_test_split(X_audio_augmented, y_audio_augmented, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4307efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_waveform_and_play_audio(y_audio_augmented[1:4:2], wavs_augmented[1:4:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13e066a",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 1000px;\n",
    "\">\n",
    "\n",
    "**Question 3:**  \n",
    "How is the amplitude (the left plot) different for the augmented samples – why?  \n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca94785a",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 1000px;\n",
    "\">\n",
    "\n",
    "**Question 4:**  \n",
    "What would happen if we increase $\\sigma$ to 100? What about `slow_down_factor` to 0.001? What if we set min and max pitch the same? Is there anything in common with these proposed augmentations?\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ebb9c",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 1000px;\n",
    "\">\n",
    "\n",
    "**Question 5:**  \n",
    "If you were to make a new data augmentation for audio, what would it be? \n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a3247",
   "metadata": {},
   "source": [
    "## Images Data Augmentation - MNIST\n",
    "\n",
    "In this part of the notebook, we will show you a more popular example of data augmentation - namely on images and the MNIST Dataset. It is where data augmentation is more commonly applied in industry and more easily understood. \n",
    "\n",
    "\n",
    "Please run all code cells below, and you will end up with an interactive menu on the MNIST dataset. There you can play around with different data augmentation and have them visualized in real time. Then, run through the theoretical exercises and try to continue by improving the list of augmentations provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4560879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, val_dataloader, epochs=3):\n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = []\n",
    "        for images, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        train_losses.append(np.mean(train_loss))\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_dataloader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        val_losses.append(val_loss / len(val_dataloader))\n",
    "    return train_losses, val_losses\n",
    "\n",
    "def test(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cee9f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "menu = retrieve_mnist_menu()\n",
    "menu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd3a9cf",
   "metadata": {},
   "source": [
    "## Exercise: Creating Your Own Data Augmentation Pipeline\n",
    "\n",
    "The time has come. After seeing our examples and playing around with how the given numbers look around, we want you to arrange your own set of augmentations. Your task is to find an augmentation to boost the performance on a training and validation set on a subset of data we offer you. \n",
    "\n",
    "You will do all of this in the `transform_aug` variables, which takes in a list of augmentations. This list is currently empty (except for the normalization step, which we leave as an exercise for the reader to know why it is there). Throughout this notebook we have been using the *Albumentations* library to do our data augmentations. This will continue to be the case here. You are free to select from any of the following functions there, and adjust their parameters. Namely:\n",
    "- `A.Rotate(float rotate, float p)`  \n",
    "- `A.Affine(float translate_x, float translate_y, float scale, float p)`  \n",
    "- `A.GaussianBlur(int gaussian_kernel_size, float blur_sigma, float p, bool gaussian_blur)`  \n",
    "- `A.HorizontalFlip(float p, bool horizontal_flip)`  \n",
    "- `A.VerticalFlip(float p, bool vertical_flip)`  \n",
    "- `A.RandomBrightnessContrast(float p, bool random_brightness_contrast)`  \n",
    "- `A.RGBShift(float p, bool rgb_shift)`  \n",
    "\n",
    "**Important**: Please take note that sometimes the function can take in both a number, a tuple, and a list!! \n",
    "\n",
    "The API of the augmentations is provided on this link https://albumentations.ai/docs/api-reference/. Be careful - the optimal combination may not be all of those augmentations at once, or with all of the parameters! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a10f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "## START ANSWER \n",
    "p=0.25\n",
    "## END ANSWER\n",
    "\n",
    "transform_aug = AlbumentationsTransform(A.Compose([\n",
    "            # Add your chosen augmentations here, e.g.:\n",
    "            # A.Rotate(limit=(float rotate), float p), p=0.\n",
    "            ## START OF ANSWER \n",
    "            ## END OF ANSWER \n",
    "            A.Normalize(mean=0.5, std=0.5, max_pixel_value=255.0),\n",
    "            \n",
    "], seed=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7eb6b4",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 1000px;\n",
    "\">\n",
    "\n",
    "**Question 6:**  \n",
    "How often would you choose to apply the data augmentation with the parameter **p**? Justify. Think what happens if the values end up between 0 and 1.\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:** \n",
    "<!-- START OF ANSWER --> \n",
    "<!-- END OF ANSWER -->\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_no_aug = AlbumentationsTransform(A.Compose([\n",
    "    A.Normalize(mean=0.5, std=0.5, max_pixel_value=255.0),\n",
    "], seed=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f181098",
   "metadata": {},
   "source": [
    "## Dataset Parameters and Visualization\n",
    "\n",
    "Here we set you the subset size you will be working on with your data augmentation from the code cell above. The expectation is that accuracy will increase for the correct set of data augmentation. Play around it - adjust the size of the dataset if you wish to or the samples per class, which attemt to balance out your distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "epochs = 100 #WARNING - IF TOO LOW, MAY NOT WORK\n",
    "\n",
    "trainset_no_aug, valset_no_aug, _, trainloader_no_aug, valloader_no_aug, testloader = load_mnist_data(train=True,data_augmentations=transform_no_aug,  subset_size=400, )\n",
    "trainset_aug, valset_aug, _,  trainloader_aug, valloader_aug, _ = load_mnist_data(train=True, data_augmentations=transform_aug, subset_size=400)\n",
    "\n",
    "visualize_pytorch_dataset_images(trainset_no_aug, \"No Data Augmentation\")\n",
    "visualize_pytorch_dataset_images(trainset_aug, \"With Data Augmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_aug = SimpleCNN()\n",
    "losses_no_aug_train, losses_no_aug_val = train(model_no_aug, trainloader_no_aug, valloader_no_aug, epochs=epochs)\n",
    "acc_no_aug = test(model_no_aug, testloader)\n",
    "\n",
    "model_aug = SimpleCNN()\n",
    "losses_aug_train, losses_aug_val = train(model_aug, trainloader_aug, valloader_no_aug, epochs=epochs)\n",
    "acc_aug = test(model_aug, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb38502",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 1000px;\n",
    "\">\n",
    "\n",
    "## Challenge: Accuracy > 88%\n",
    "\n",
    "We have provided you a data augmentation setup for a limited subset size. You have the full capability to improve performance on the MNIST task. We challenge you to reach an accuracy of above 88%. You can add whatever data augmentations you want, change the probabilities of the augmentation **p**, or increase training time. To get test sense performance, run the cell below.\n",
    "\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cffe2",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 1000px;\n",
    "\">\n",
    "\n",
    "**Question 7:**  \n",
    "What data augmentations did you choose and why? Are there any data augmentations you did NOT choose? If so, why? Feel free to come back to this questions after experiemnting.\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7894ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.show_metrics import compare_two_losses\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"Accuracy without augmentation: {acc_no_aug:.4f}\")\n",
    "print(f\"Accuracy with augmentation: {acc_aug:.4f}\")\n",
    "\n",
    "def compare_two_losses(loss_a, loss_a_val, loss_a_label, loss_b, loss_b_val, loss_b_label):\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(loss_a, label=loss_a_label)\n",
    "    plt.plot(loss_a_val, label=loss_a_label + ' (val)')\n",
    "    plt.plot(loss_b, label=loss_b_label)\n",
    "    plt.plot(loss_b_val, label=loss_b_label + ' (val)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.title('Loss Comparison')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_accuracy_comparison(acc_no_aug_list, acc_aug_list):\n",
    "    accuracies = [acc_no_aug_list[-1], acc_aug_list[-1]]\n",
    "    labels = ['No Data Augmentation', 'With Data Augmentation']\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(labels, accuracies, color=['red', 'green'])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Accuracy Comparison')\n",
    "\n",
    "    for i, acc in enumerate(accuracies):\n",
    "        plt.text(i, acc + 0.02, f\"{acc:.4f}\", ha='center')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "compare_two_losses(losses_no_aug_train, losses_no_aug_val, 'No Data Augmentation', losses_aug_train, losses_aug_val, \"With Data Augmentation\")\n",
    "plot_accuracy_comparison([acc_no_aug], [acc_aug])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b359eadc",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 1000px;\n",
    "\">\n",
    "\n",
    "**Question 8:**  \n",
    "Examine the training loss over different epochs (both validation and training) on the augmented and non-augmented data. What difference do you see? Why do you see it? What about the validation loss?\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "The augmented training loss is much less stable than the non-augmented one. This makes sense as augmentations change over forward passes and the task is overall much harder. The validation loss on the non-augmented set starts rising. This indicates overfitting.\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2905c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_sizes = [80, 160, 320, 640, 1280]  # Different subset sizes to try\n",
    "epochs = 100 #WARNING - IF TOO LOW, MAY NOT WORK\n",
    "\n",
    "acc_no_aug_list = []\n",
    "acc_aug_list = []\n",
    "\n",
    "for subset_size in subset_sizes:\n",
    "    trainset_no_aug, valset_no_aug, _, trainloader_no_aug, valloader_no_aug, testloader = load_mnist_data(train=True,data_augmentations=transform_no_aug,  subset_size=subset_size, )\n",
    "    trainset_aug, valset_aug, _,  trainloader_aug, valloader_aug, _ = load_mnist_data(train=True, data_augmentations=transform_aug, subset_size=subset_size)\n",
    "    \n",
    "    model_no_aug_curr = SimpleCNN()\n",
    "    losses_no_aug_train, losses_no_aug_val = train(model_no_aug_curr, trainloader_no_aug, valloader_no_aug, epochs=epochs)\n",
    "    acc_no_aug_curr = test(model_no_aug_curr, testloader)\n",
    "\n",
    "    model_aug_curr = SimpleCNN()\n",
    "    losses_aug_train, losses_aug_val = train(model_aug_curr, trainloader_aug, valloader_no_aug, epochs=epochs)\n",
    "    acc_aug_curr = test(model_aug_curr, testloader)\n",
    "\n",
    "    acc_no_aug_list.append(acc_no_aug_curr)\n",
    "    acc_aug_list.append(acc_aug_curr)\n",
    "\n",
    "plot_accuracy_vs_subset_size(\n",
    "    subset_sizes=subset_sizes,\n",
    "    acc_no_aug=acc_no_aug_list,\n",
    "    acc_with_aug=acc_aug_list,\n",
    "    title='Performance: Impact of Data Augmentation vs. Subset Size'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7244f2a",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 650px;\n",
    "\">\n",
    "\n",
    "**Question 9:**  \n",
    "Reason about the performance on the augmented data over the different subset sizes on the plot.\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a82ea4",
   "metadata": {},
   "source": [
    "### Contrail Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5100ff",
   "metadata": {},
   "source": [
    "Now comes the time to apply our data augmentations to real data, not just a toy problem like MNIST. \n",
    "\n",
    "We are in luck, however! We can (mostly) just reapply all of the data augmentations we have done to MNIST. The following section details how data augmentation is applied and lets you play with another set of augmentations to improve contrail performance. \n",
    "\n",
    "Alternatively, you could also take a look at our set of data augmentatiosn they are not necessarily the best set of augmentations for all cases. Please answer all of the theoretical questions, and run through the code. Our configuration of data augmentations is not necessarily optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de254f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from visualization.interactive_menu import retrieve_contrails_menu\n",
    "menu = retrieve_contrails_menu()\n",
    "menu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ab47f",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 650px;\n",
    "\">\n",
    "\n",
    "**Question 10:**  \n",
    "Is the data augmentations on the contrails different algorithmically? What has changed?\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43010aea",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 650px;\n",
    "\">\n",
    "\n",
    "**Question 11:**  \n",
    " Investigate what the padding option does. Discuss the different padding options and discuss which one you think is most suitable for the contrails. \n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb34bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=0.5\n",
    "\n",
    "transform_contrails_aug = A.Compose([\n",
    "            A.Resize(128, 128),\n",
    "            A.Rotate(limit=(-1.0*15, 15), border_mode=cv2.BORDER_REFLECT_101, p=0.5),\n",
    "            A.Affine(translate_percent={'x': 0.1, 'y': 0.1}, border_mode=cv2.BORDER_REFLECT_101, p=p),\n",
    "            A.GaussianBlur(blur_limit=3, sigma_limit=2, p=p),\n",
    "            A.RandomBrightnessContrast(p=p),\n",
    "            A.Perspective(scale=(0.05, 0.1), border_mode=cv2.BORDER_REFLECT_101, p=p),\n",
    "            A.RGBShift(p=p),\n",
    "            A.Normalize(mean=0.5, std=0.5, max_pixel_value=255.0),\n",
    "        ])\n",
    "\n",
    "transform_no_aug_contrails = A.Compose([\n",
    "    A.Resize(128, 128),\n",
    "    A.Normalize(mean=0.5, std=0.5, max_pixel_value=255.0),\n",
    "])\n",
    "\n",
    "\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbc544",
   "metadata": {},
   "source": [
    "#### Training the Models\n",
    "\n",
    "The following code cells train the augmentations you came up with on the actual models - you are free to leave them to train over the break and then come back to them to see how the loss behaves with different validations. In the interest of time, however, we have pre-trained models for the later part of the session and provided the loss plots instead for the no-augmentation and augmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_contrail_network(transform_no_aug_contrails, EPOCHS, \"dice\", \"base\", dataset=\"own\", experiment_name=\"no_augmentation-dice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e20ee6",
   "metadata": {},
   "source": [
    "![image.png](https://raw.githubusercontent.com/vdakov/contrail-segmentation/dev/images/contrail-loss-no-aug-200.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f532c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_contrail_network(transform_contrails_aug, EPOCHS, \"dice\", \"base\", dataset=\"own\", experiment_name=\"augmentation-dice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f8f08",
   "metadata": {},
   "source": [
    "![image.png](https://raw.githubusercontent.com/vdakov/contrail-segmentation/dev/images/contrail-loss-aug-200.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc687ba4",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 650px;\n",
    "\">\n",
    "\n",
    "**Question 12:**  \n",
    "Compare the loss plots of the no-augmentation vs. augmentation model. What is different? What about their predictions?\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0f958",
   "metadata": {},
   "source": [
    "#### Are these predictions even good? \n",
    "\n",
    "Right now we have two trained contrail models - however, we have not defined ways to evaluate them. The reason is that getting a notion of how good the contrails model (or any deep learning model in industry for that matter) is hard. Here we show a naive approach of defining our metrics as \"the number of matching pixels\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4746478",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 650px;\n",
    "\">\n",
    "\n",
    "**Question 13:**  \n",
    "Run the code cells below. What do you see about these predictions? Are these notions of accuracy reasonable?\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def evaluate_binary_image(input_img, label_img, threshold=0.5):\n",
    "    # --- Load input image ---\n",
    "    if isinstance(input_img, str):\n",
    "        input_img = np.array(Image.open(input_img).convert('L')) / 255.0\n",
    "    elif isinstance(input_img, np.ndarray) and input_img.ndim == 3:\n",
    "        input_img = np.mean(input_img, axis=2)\n",
    "\n",
    "    # --- Load label image (RGBA) ---\n",
    "    if isinstance(label_img, str):\n",
    "        label_img_pil = Image.open(label_img).convert('RGBA')\n",
    "        label_arr = np.array(label_img_pil)\n",
    "    elif isinstance(label_img, np.ndarray):\n",
    "        label_arr = label_img\n",
    "\n",
    "    # Alpha mask: 1 for visible pixels, 0 for transparent\n",
    "    alpha_mask = label_arr[..., 3] > 0\n",
    "\n",
    "    # Black pixels as label = 1\n",
    "    black_mask = (label_arr[..., :3].sum(axis=2) == 0)\n",
    "    label_bin = np.zeros(black_mask.shape, dtype=np.float32)\n",
    "    label_bin[black_mask & alpha_mask] = 1.0  # only visible black pixels\n",
    "\n",
    "    pred_bin = (input_img > threshold).astype(np.float32)\n",
    "\n",
    "    accuracy = np.mean(pred_bin == label_bin)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(input_img, cmap='gray')\n",
    "    plt.title(\"Input / Prediction\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(label_bin, cmap='gray')\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    overlay = np.zeros((*pred_bin.shape, 3))\n",
    "    overlay[..., 0] = label_bin  # red = GT\n",
    "    overlay[..., 1] = pred_bin   # green = prediction\n",
    "    overlay[..., 2] = label_bin * pred_bin  # blue = overlap\n",
    "    plt.imshow(overlay, cmap='gray')\n",
    "    plt.title(f\"Overlay (GT=R, Pred=G)\\nAccuracy={accuracy:.3f}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63be7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_binary_image(np.zeros((235, 355)), \"contrail-seg/data/goes/florida/mask/florida_2020_03_05_0101.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69fdfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_binary_image(np.ones((235, 355)), \"contrail-seg/data/goes/florida/mask/florida_2020_03_05_0101.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec5e810",
   "metadata": {},
   "source": [
    "In the following section, you will learn about how you can better evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3877f",
   "metadata": {},
   "source": [
    "---\n",
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536309a7",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/vdakov/contrail-segmentation/dev/images/confusion_matrix.png)\n",
    "\n",
    "\n",
    "\n",
    "\"[Diagnostic testing diagram](https://en.wikipedia.org/wiki/Template:Diagnostic_testing_diagram)\" by Wikipedia contributors / [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427e7bd4",
   "metadata": {},
   "source": [
    "## Drawing Labels Yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb239a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from widgets.precision_recall_paint import Painter \n",
    "IMAGE_PATH = \"contrail-seg/data/goes/florida/image/florida_2020_03_05_0101.png\"  \n",
    "LABEL_PATH = \"contrail-seg/data/goes/florida/mask/florida_2020_03_05_0101.png\"\n",
    "\n",
    "# Run the tool\n",
    "p = Painter(IMAGE_PATH, LABEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e09bee",
   "metadata": {},
   "source": [
    "### Comparing models\n",
    "\n",
    "Now you have seen various ways to evaluate the models. Let's try to get our hands dirty with some actual models and their predictions:\n",
    "\n",
    "| Metric        | Model A  | Model B|\n",
    "| ------------- | ------------------------------------ | ------------------------------------ |\n",
    "| **Accuracy**  | 0.94                                 | 0.89                                 |\n",
    "| **Precision** | **0.92**                             | 0.48                                 |\n",
    "| **Recall**    | 0.31                                 | **0.88**                             |\n",
    "| **F1 Score**  | 0.46                                 | **0.62**                             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eba134",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 1000px;\n",
    "\">\n",
    "\n",
    "**Question 14:**  \n",
    "You see the following model image, based on the input and label below. Judging off the metrics, which model does this belong to most likely - A or B? Fill it in for Model 1 and Model 2 below. \n",
    "<table>\n",
    "  <tr>\n",
    "    <td><b>Input</b><br><img src=\"https://raw.githubusercontent.com/vdakov/contrail-segmentation/dev/images/input-image-ex16.png\" width=\"150\"></td>\n",
    "    <td><b>Label</b><br><img src=\"https://raw.githubusercontent.com/vdakov/contrail-segmentation/dev/images/ex-16-label-image.png\" width=\"150\"></td>\n",
    "    <td><b>Model Prediction 1</b><br><img src=\"https://raw.githubusercontent.com/vdakov/contrail-segmentation/dev/images/ex-16-model-1.png\" width=\"150\"></td>\n",
    "    <td><b>Model Prediction 2</b><br><img src=\"https://raw.githubusercontent.com/vdakov/contrail-segmentation/dev/images/ex-16-model-2.png\" width=\"150\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8321f028",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 1000px;\n",
    "\">\n",
    "\n",
    "**Question 15:**  \n",
    "Which model would you prefer for the contrail task? A or B? Why?\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6dd56",
   "metadata": {},
   "source": [
    "### Evaluating the pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a3688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_no_aug = \"data/models/no_augmentation-dice-own-dice_base-200epoch.torch\"\n",
    "evaluate_contrail_model(model_path_no_aug)\n",
    "_ = plot_pr_curve(model_path_no_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd5a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_aug = \"data/models/augmentation-dice-own-dice_base-200epoch.torch\"\n",
    "evaluate_contrail_model(model_path_aug)\n",
    "_ = plot_pr_curve(model_path_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df0b50",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 1000px;\n",
    "\">\n",
    "\n",
    "**Question 16:**  \n",
    "The differences between the two real models are subtle. Describe what you see qualitatively between the two models. Which one would you prefer and why?\n",
    "\n",
    "Can you think of a more suitable metric than precision and recall for comparing the two? Think once again about the task we have and why we do it: \n",
    "- We want to know how much contrails are in an image \n",
    "- We want to know where there contrails are\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6ffa0",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b70bc",
   "metadata": {},
   "source": [
    "# Ethics: Responsibilities \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae1679",
   "metadata": {},
   "source": [
    "You have seen parts of the model pipeline already. You know how to think about critical aspects of your data before model training, during evaluation, and even more. \n",
    "\n",
    "**But what about how that model would be used**?   Unfortunately, in the real world, an application is rarely as simple as deploying the model - somebody is responsible for the effects caused by it - economic, social, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f09269",
   "metadata": {},
   "source": [
    "Assume that this assignment was provided by a stakeholder – say the Dutch government – because they want to use your tool to reduce this surface area. **The funder assumes that the minimization of contrail surface is good for the climate**. This means that it has effect on counteracting global warming. It is why they have given you this assigment. They want to deploy such a model to initialte policies that will reduce contrail surface area. This might result in the **prohibitation** of certain flight routes, for example. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076cb8b7",
   "metadata": {},
   "source": [
    "Unfortunately, research shows this may not be always the case, and there is some uncertainty about these effects. ([https://acp.copernicus.org/articles/24/9219/2024/](https://eur03.safelinks.protection.outlook.com/?url=https%3A%2F%2Facp.copernicus.org%2Farticles%2F24%2F9219%2F2024%2F&data=05%7C02%7CM.Sand%40tudelft.nl%7Cc76d0589bd4945603f1708de0d71de43%7C096e524d692940308cd38ab42de0887b%7C0%7C0%7C638962979860328000%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=5CRoOnZviMYuDuqLT95zhF5r6S2O1n57RSHA8HuTbRQ%3D&reserved=0)): Since the physical processes that underlie these assumptions are very complex, there is a possibility that the contrary could be true. It could be the case that the bigger the surface area of the contrails, the more sun is being reflected before reaching and warming earth’s surface, which might in fact cause a “cooling effect” and is, therefore, beneficial for the climate. This uncertainty is difficult to quantify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c9045",
   "metadata": {},
   "source": [
    "This poses an ethical challenge that we want you to discuss now: **What are your responsibilities as engineers in this regard?**\n",
    "\n",
    "Do you have to report the uncertainties that you know about based on the research you have done in this project? Stronger even: Do you have a responsibility to discourage the intended use by the stakeholder (the government)?\n",
    "\n",
    "\n",
    "In moral philosophy, we make an important distinction, which shall guide the first steps of your discussion. It is the distinction between backward- and forward-looking responsibility. Forward-looking responsibilities are responsibilities to ensure that certain negative events – harm, or damage – do not occur. Those are sometimes also named “duties” or “obligations”. But duties and obligations do not completely exhaust the concept of forward-looking responsibility: There are also certain character traits that are expected from people, which are forward-looking, for instance, to be faithful in relationships. For scientists, they need to be truthful, honest and reflexive – all of these are forward-looking virtues.\n",
    "\n",
    "Backward-looking responsibilities – on the other hand – are invoked, when things have gone wrong, when damage has occurred or harm has been done. Then the harmed parties or third parties – the state, legal institutions – will hold one accountable. That means you have to answer for your actions, testify to whether you have known that those outcomes ensue or whether you might have even wanted those outcomes to come about directly or as a product of your actions.\n",
    "\n",
    "In many case, in which someone has ignored a forward-looking responsibility and harm is caused, one is held accountable and made liable – one will be punished or blamed (held backward-looking responsible).\n",
    "\n",
    "This roughly sketched conceptual distinctions shall inform some exercises that are best done in groups.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d3e0e",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 1000px;\n",
    "\">\n",
    "\n",
    "**Question 17:**  \n",
    "What is the difference between forward- and backward-looking responsibility? Provided an example for a machine learning case. If you have a partner, explain it to each other.\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964920e4",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 1000px;\n",
    "\">\n",
    "\n",
    "**Question 18:**  \n",
    "What kind of responsibility (active or passive) is at stake in the case just sketched?\n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef0f3d",
   "metadata": {},
   "source": [
    "Now the ball is in your court. What is your responsibility as an engineer? Please consult the following document: [Code of Ethics:The Royal Netherlands Society of Engineers](https://kivi.nl/_Resources/Persistent/5/3/3/8/5338c4ad1a3d5b95098c8313aec95307d29b4058/KIVI%20Code%20of%20Ethics%202018.pdf). It outlines basic responsibilities all engineers in the Netherlands should be familiar with. The principles apply to other countries as well. \n",
    "\n",
    "Your task is to read the document, think on the outlined responsibilities and relate them to the contrails case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc6ea3",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "  font-family: Arial, sans-serif;\n",
    "  background: #00e5ff;\n",
    "  color: #0b1220;\n",
    "  padding: 15px 20px;\n",
    "  border-radius: 12px;\n",
    "  line-height: 1.6;\n",
    "  max-width: 1000px;\n",
    "\">\n",
    "\n",
    "**Question 19:**  \n",
    "With your partner (if you have one), flip a coin and pick a thesis to defend from the ones below. If you are doing this alone, pick which thesis you agree with and write the arguments why while using the code of ethics. \n",
    "\n",
    "**Position 1**: Engineers are responsible for sharing possible uncertainties regarding related research that might impact how their products are being used by stakeholders. \n",
    " \n",
    "**Position 2**: Engineering teams are not responsible for sharing uncertainties that are related to their work, if it is not an integral part of the model itself. No matter the possible and predictably negative consequences of the utilization of their model. \n",
    "\n",
    "---\n",
    "\n",
    "**Answer:**  \n",
    "<!-- START OF ANSWER -->\n",
    "<!-- END OF ANSWER -->\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125035a8",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Engineering projects are undertaken in the real world – they do not only describe and understand the world, they get initiated to change the world in important ways: For instance, by finding out, what better flight routes there are for airplanes and whether and how to impact current flight behavior. \n",
    "\n",
    "Because of that, engineering projects are always ethically relevant. They have social and environmental impacts; positive impacts if they are undertaken responsibly, or negative impacts if they are undertaken irresponsibly. \n",
    "\n",
    "Today, you have learned about the data selection process in machine learning, how we can make sure our models work well, and also that there are Codes of Conduct, established by professional engineering associations around the world. All of this should guide engineers to becoming better professionals. \n",
    "\n",
    "These documents – if you are uncertain about your responsibilities – can help you making better decisions. Oftentimes, however, their meaning must be continuously reinterpreted in light of new technologies and applications and interpretated in relation to actual cases: What does it mean to be transparent about uncertainty in the context of computational science? Who are the relevant stakeholders that need to be made aware of some consequences? How can we respect all cultural values equally in global engineering projects? \n",
    "\n",
    "To determine the meaning of these obligations and their scope, requires constant re-evaluation. For this, teamwork, a respectful and open dialogue about the scope and meaning of those terms is paramount. \n",
    "\n",
    "Such discussions will inadvertently also re-occur in your future professions. Today, you have practiced such dialogues in relation to a concrete case. We hope you have enjoyed this exercise; maybe the discussion changed your mind or let you see things with different eyes! That is the first step towards morally responsible engineering. \n",
    "\n",
    "#### Congratulations for rounding up this module! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
